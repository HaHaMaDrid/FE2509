---
title: "PCA_OLS"
author: "Wu Fuheng"
date: "Sunday, November 02, 2014"
output: html_document
---

Over at stats.stackexchange.com recently, a really interesting question was raised about principal component analysis (PCA). The gist was “Thanks to my college class I can do the math, but what does it MEAN?"

I felt like this a number of times in my life. Many of my classes were focused on the technical implementations they kinda missed the section titled “Why I give a shit." A perfect example was my Mathematics Principles of Economics class which taught me how to manually calculate a bordered Hessian but, for the life of me, I have no idea why I would ever want to calculate such a monster. OK, that’s a lie. Later in life I learned that bordered Hessian matrices are a second derivative test used in some optimizations. Not that I would EVER do that shit by hand. I’d use some R package and blindly trust that it was coded properly.

So back to PCA: as I was reading the aforementioned stats question I was reminded of a recent presentation that Paul Teetor gave at a August Chicago R User Group. In his presentation on spread trading with R he showed a graphic that illustrated the difference between OLS and PCA. I took some notes and went home and made sure I could recreate the same thing. If you have wondered what makes OLS and PCA different, open up an R session and play along.

__Your Independent Variable Matters:__

The first observation to make is that regressing x ~ y is not the same as y ~ x even in a simple univariate regression. You can illustrate this by doing the following:
```{r, eval=TRUE, results='hide', echo=TRUE}
set.seed(2)
x <- 1:100
y <- 20 + 3 * x
e <- rnorm(100, 0, 60)
y <- 20 + 3 * x + e
```

You should get something that looks like this:
```{r, eval=TRUE, results='hide', echo=TRUE, fig.align='left'}
plot(x,y)
yx.lm <- lm(y ~ x)
lines(x, predict(yx.lm), col="red")

xy.lm <- lm(x ~ y)
lines(predict(xy.lm), y, col="blue")
```


So it’s obvious they give different lines. But why? Well, OLS minimizes the error between the dependent and the model. Two of these errors are illustrated for the y ~ x case in the following picture:

<img src=http://www.cerebralmastication.com/wp-content/uploads/2010/09/OLS1.png>

<img src=http://www.cerebralmastication.com/wp-content/uploads/2010/09/OLS2.png>

But when we flip the model around and regress x ~ y then OLS minimizes these errors:



Ok, so what about PCA?

Well let’s draw the first principal component the old school way:
```{r, eval=TRUE, echo=TRUE, fig.align='left'}
#normalize means and cbind together
xmean=mean(x)
ymean=mean(y)
xyNorm <- cbind(x=x-xmean, y=y-ymean)
plot(xyNorm)

#covariance
xyCov <- cov(xyNorm)
eigenValues <- eigen(xyCov)$values
eigenVectors <- eigen(xyCov)$vectors
eigenValues
eigenVectors
```

PCA minimizes the error orthogonal (perpendicular) to the model line. So first principal component looks like this:
<img src=http://www.cerebralmastication.com/wp-content/uploads/2010/09/pca.png>


The two yellow lines, as in the previous images, examples of two of the errors which the routine minimizes.

So if we plot all three lines on the same scatter plot we can see the differences:

```{r, eval=TRUE, results='markup', echo=TRUE, fig.align='left', fig.height=6, fig.width=6}
#plot(xyNorm) #, ylim=c(-200,200), xlim=c(-100,100)
plot(xyNorm, ylim=c(-200,200), xlim=c(-200,200))
lines(xyNorm[x], eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x], col='purple')
lines(xyNorm[x], eigenVectors[2,2]/eigenVectors[1,2] * xyNorm[x], col='red')

# the two PCAs are perpendicular
(atan(eigenVectors[2,1]/eigenVectors[1,1]) - atan(eigenVectors[2,2]/eigenVectors[1,2]))*180/pi
eigenVectors[,1] %*% eigenVectors[,2]
```

The x ~ y OLS and the first principal component are pretty close, but not exactly the same.

```{r, eval=TRUE, results='hide', echo=TRUE, fig.align='left'}
# the largest eigenValue is the first one
# so that’s our principal component.
# but the principal component is in normalized terms (mean=0)
# and we want it back in real terms like our starting data
# so let’s denormalize it
plot(x,y)
lines(x, (eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x]) + mean(y), col='purple')
# that looks right. line through the middle as expected

# what if we bring back our other two regressions?
lines(x, predict(yx.lm), col="red")
lines(predict(xy.lm), y, col="blue")
```

And the final data is:

```{r, eval=TRUE, echo=TRUE, fig.align='left'}
FinalData = t(eigenVectors[,1]) %*% t(xyNorm)
#Final Data After dimension reduction
plot(FinalData[1,])
#Trying to get databack but the retrived data have lost some info
plot(t((eigenVectors[,1]) %*% FinalData))


FinalData2 = t(eigenVectors) %*% t(xyNorm)
head(t((eigenVectors) %*% FinalData2))
head(xyNorm)
```